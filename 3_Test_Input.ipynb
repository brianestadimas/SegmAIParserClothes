{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import Subset\n",
    "import shutil\n",
    "from codes.helpers import FocalLoss\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_name_to_load = \"final_model.pth\"\n",
    "\n",
    "batch_size = 1\n",
    "device = \"cuda\"\n",
    "logdir = \"logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Fashion_Data(Dataset):\n",
    "    def __init__(self, folder_train, augmentation=None):\n",
    "        self.folder_train = folder_train\n",
    "        self.transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()]) if augmentation is None \\\n",
    "                            else transforms.Compose([augmentation, transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "        \n",
    "        self.filenames = [f for f in os.listdir(folder_train) if os.path.isfile(os.path.join(folder_train, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name_train = os.path.join(self.folder_train, self.filenames[idx])\n",
    "\n",
    "        img_train = Image.open(img_name_train).convert('RGB')\n",
    "        img_train = self.transform(img_train)\n",
    "\n",
    "        return img_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "test_folder = \"logs/input/\"\n",
    "test_set = Fashion_Data(test_folder)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The diagram is provided to understand the architecture\n",
    "from architecture.segnet import SegNet\n",
    "\n",
    "\n",
    "generator = SegNet(in_channels=3).to(device)\n",
    "generator.load_state_dict(torch.load(f'logs/{generator_name_to_load}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# The code to make sure the model input and output are correct, we can test the first batch to fed into the network\n",
    "data = next(iter(test_dataloader))\n",
    "\n",
    "test_data = data\n",
    "output_test = generator(test_data.to(device))\n",
    "print(output_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "generator.eval()\n",
    "pbar = tqdm(test_dataloader)\n",
    "for idx, x in enumerate(pbar):\n",
    "    x = x.to(device, dtype=torch.float32)\n",
    "    output = generator(x)\n",
    "    for j, gen_image in enumerate(output):\n",
    "        save_image(gen_image, os.path.join(\"logs\", \"output\", f\"generated_{idx}_{j}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_and_recolor_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    data = np.array(img)\n",
    "\n",
    "    # Check if image has an alpha channel; if so, ignore it in processing\n",
    "    if data.shape[2] == 4:\n",
    "        data = data[:, :, :3]\n",
    "\n",
    "    height, width, _ = data.shape\n",
    "    # Initialize a mask to keep track of already processed (recategorized) pixels\n",
    "    processed_mask = np.zeros((height, width), dtype=bool)\n",
    "\n",
    "    threshold_rg = 5  # Threshold for R-G and R-B for red-ish, including browns and purples\n",
    "    threshold_w = 220  # Threshold for identifying white-ish pixels\n",
    "    threshold_yg = 5  # Additional threshold for yellow, ensuring G is close to R and B is low\n",
    "\n",
    "    # Define functions for each color categorization to improve readability\n",
    "    def apply_mask(condition, new_color):\n",
    "        nonlocal processed_mask\n",
    "        mask = condition & ~processed_mask\n",
    "        data[mask] = new_color\n",
    "        processed_mask |= mask\n",
    "\n",
    "    # White-ish\n",
    "    white_condition = (data[:, :, 0] > threshold_w) & (data[:, :, 1] > threshold_w) & (data[:, :, 2] > threshold_w)\n",
    "    apply_mask(white_condition, [5, 5, 5])\n",
    "\n",
    "    # Yellow-ish\n",
    "    yellow_condition = (data[:, :, 0] > data[:, :, 2] + threshold_yg) & (np.abs(data[:, :, 0] - data[:, :, 1]) < threshold_yg)\n",
    "    apply_mask(yellow_condition, [25, 25, 25])\n",
    "\n",
    "    # Red-ish (including browns and purples)\n",
    "    red_condition = (data[:, :, 0] > data[:, :, 1] + threshold_rg) & (data[:, :, 0] > data[:, :, 2] + threshold_rg)\n",
    "    apply_mask(red_condition, [22, 22, 22])\n",
    "\n",
    "    # Green-ish\n",
    "    green_condition = (data[:, :, 1] > data[:, :, 0] + threshold_rg) & (data[:, :, 1] > data[:, :, 2] + threshold_rg)\n",
    "    apply_mask(green_condition, [21, 21, 21])\n",
    "\n",
    "    # Blue-ish\n",
    "    blue_condition = (data[:, :, 2] > data[:, :, 0] + threshold_rg) & (data[:, :, 2] > data[:, :, 1] + threshold_rg)\n",
    "    apply_mask(blue_condition, [24, 24, 24])\n",
    "    \n",
    "    \n",
    "    # Convert back to PIL Image for output\n",
    "    new_img = Image.fromarray(data)\n",
    "    return new_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
