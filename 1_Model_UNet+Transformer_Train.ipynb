{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 13:16:43.252105: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-18 13:16:43.253669: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-18 13:16:43.286069: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-18 13:16:44.007738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from codes.metrics import relative_root_mean_squared_error, r_score\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file containing DICOM file paths and masses\n",
    "training_root = 'New_Target_Trainng Data.xlsx'\n",
    "test_root = 'New_Target_Test Data.xlsx'\n",
    "root_folder = 'New_Training_Data_FINAL/'\n",
    "root_folder_test = 'New_Test Data/'\n",
    "df = pd.read_excel(training_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# We can adjust the learning_rate here to improve the model performance, scheduler is used to reduce the learning rate at specific epochs\n",
    "# The batch size can be adjusted based on the available memory\n",
    "# If apply_augmentations is set to True, the model will apply augmentations to the images to improve the model performance\n",
    "load_saved_model = False\n",
    "apply_augmentations = True\n",
    "training_epoch = 6000\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "scheduler = 1\n",
    "\n",
    "device = \"cuda:1\"\n",
    "logdir = \"logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Data class is used to load the DICOM files and masses from the Excel file\n",
    "\n",
    "\n",
    "class DC_Data(Dataset):\n",
    "    def __init__(self, root_csv, root='', augmentation=None, png=False):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object.\n",
    "        \n",
    "        Parameters:\n",
    "        - root_csv: Path to the Excel file containing filenames and mass values.\n",
    "        - root: Root directory where the DICOM files are located.\n",
    "        - augmentation: Transformations to be applied to the images. If None, a default resize transformation is applied.\n",
    "        - png: A boolean flag indicating whether the images are stored in PNG format instead of DICOM. This is primarily used to load the segmentation masks\n",
    "        as the augmented data from unsupervised segmentation model.\n",
    "        \n",
    "        Note: The file codes/data.py contains the DC_Data class which is used to load the DICOM files and masses from the Excel file, however\n",
    "        it extended with the depth and inverse depth prediction tasks.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_excel(root_csv)\n",
    "        self.root = root\n",
    "        self.png = png\n",
    "        self.transform = transforms.Compose([transforms.Resize((256, 256))]) if augmentation is None \\\n",
    "                            else transforms.Compose([augmentation, transforms.Resize((256, 256))]) \n",
    "        if self.png:\n",
    "            self.df['image_path'] = self.df.apply(lambda x: os.path.join(self.root, x[0].replace('.dcm', '.png')), axis=1)\n",
    "            self.df = self.df[self.df['image_path'].apply(os.path.exists)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.png:\n",
    "            # Load the image directly as a PNG\n",
    "            image_path = self.root + self.df.iloc[index, 0].replace('.dcm', '.png')\n",
    "            image = Image.open(image_path).convert('L')\n",
    "        else:\n",
    "            dicom_path = self.root + self.df.iloc[index, 0]\n",
    "            dicom_data = pydicom.dcmread(dicom_path)\n",
    "            slope = dicom_data.RescaleSlope if \"RescaleSlope\" in dicom_data else 1\n",
    "            intercept = dicom_data.RescaleIntercept if \"RescaleIntercept\" in dicom_data else 0\n",
    "            image = dicom_data.pixel_array.astype(np.float32) * slope + intercept\n",
    "            image = torch.from_numpy(image.transpose(0,1)/255)\n",
    "            image = image.unsqueeze(0)\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = transforms.functional.to_tensor(image)\n",
    "        \n",
    "        mass = self.df.iloc[index, 1]\n",
    "        \n",
    "        # The return is expected to be [1, 256, 256] and [1] for the image and mass respectively\n",
    "        return image, mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1563, Test size: 29\n"
     ]
    }
   ],
   "source": [
    "from codes.helpers import ScaleToCenterTransform, tensor_to_numpy\n",
    "\n",
    "train_set = DC_Data(training_root, root=root_folder)\n",
    "\n",
    "if apply_augmentations:\n",
    "    # Geometry Augmentation, horizontal flip, grayscale, and zoom\n",
    "    zoom_augmentation = ScaleToCenterTransform(output_size=256, scale_factor=1.5)\n",
    "    augmented_set1 = DC_Data(training_root, root=root_folder, augmentation=transforms.RandomHorizontalFlip(p=0.5))\n",
    "    augmented_set2 = DC_Data(training_root, root_folder, augmentation=transforms.RandomRotation(degrees=45))\n",
    "    augmented_set3 = DC_Data(training_root, root_folder, augmentation=transforms.Grayscale(num_output_channels=1))\n",
    "    augmented_set4 = DC_Data(training_root, root_folder, augmentation=zoom_augmentation)\n",
    "    augmented_set5 = DC_Data(training_root, root_folder, augmentation=transforms.RandomAffine(degrees=45, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=15))\n",
    "\n",
    "    # ROI Augmention, we have to define the ROI using 4_Generate_GT_Segmentation.ipynb and manully save into the Training_Generated/Mask folder\n",
    "    mask_folder = 'Training_Generated/Mask/'\n",
    "    augmented_set6 = DC_Data(training_root, mask_folder, augmentation=transforms.ToTensor(), png=True)\n",
    "    \n",
    "    # Concat the original and augmented datasets\n",
    "    train_set = ConcatDataset([train_set, augmented_set1, augmented_set2, augmented_set3, augmented_set4, augmented_set5, augmented_set6])\n",
    "\n",
    "# DataLoader is used to load the data in batches\n",
    "dataloader_train = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "test_set = DC_Data(test_root, root=root_folder_test)\n",
    "dataloader_test = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "print(f'Train size: {len(train_set)}, Test size: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256]) 0.26152158010240906\n",
      "torch.Size([1, 256, 256]) 0.25422173274596016\n",
      "torch.Size([1, 256, 256]) 0.2544004400440044\n",
      "torch.Size([1, 256, 256]) 0.28149076394690364\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    image, mass = train_set[i] \n",
    "    print(image.shape, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The architecture code is defined in file architecture/fused_transformer.py\n",
    "# The diagram is provided to understand the architecture\n",
    "from architecture.fused_transformer import UNet\n",
    "\n",
    "model = UNet(n_channels=1, n_classes=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "writer = SummaryWriter(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "# The code to make sure the model input and output are correct, we can test the first batch to fed into the network\n",
    "data = next(iter(dataloader_train))\n",
    "\n",
    "test_data = data[0]\n",
    "output_test = model(test_data.to(device))\n",
    "print(output_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class Engine(object):\n",
    "    def __init__(self, model, optimizer, device, ema=None):\n",
    "        # Initialize the Engine with the model, optimizer, and the device it's running on.\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        # Current epoch of training.\n",
    "        self.cur_epoch = 0\n",
    "        # Number of iterations the training has run.\n",
    "        self.cur_iter = 0\n",
    "        # The best validation epoch, used to track the epoch with the best validation performance.\n",
    "        self.bestval_epoch = 0\n",
    "        # Lists to track the training and validation losses.\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        # Criterion for calculating loss. Here, it's Mean Squared Error Loss for regression tasks.\n",
    "        self.criterion = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    \"\"\" Block to begin training \"\"\"\n",
    "    def train(self, dataloader_train):\n",
    "        loss_epoch = 0.\n",
    "        num_batches = 0\n",
    "        # Set the model to training mode.\n",
    "        self.model.train()\n",
    "        \n",
    "        # Train loop\n",
    "        # tqdm is used to display the training progress for each epoch.\n",
    "        predictions = []\n",
    "        ground_truths = []\n",
    "        pbar = tqdm(dataloader_train, desc='Train Epoch {}'.format(self.cur_epoch))\n",
    "        for data in pbar:\n",
    "            # efficiently zero gradients\n",
    "            # Zero the gradients before running the backward pass.\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            images = data[0].to(self.device, dtype=torch.float32)   # Image that will be fed into network\n",
    "            gt_mass = data[1].to(self.device, dtype=torch.float32)  # Ensure gt_mass is a float tensor, The ground truth of mass\n",
    "\n",
    "            # Pass the images through the model to get predictions.\n",
    "            pred_mass = self.model(images)\n",
    "\n",
    "            # The output should have the same dimensions as the target\n",
    "            if pred_mass.ndim > gt_mass.ndim:\n",
    "                pred_mass = pred_mass.squeeze()\n",
    "\n",
    "            # Calculate the loss, backpropagation, and optimization\n",
    "            loss = self.criterion(pred_mass, gt_mass)\n",
    "            loss.backward()\n",
    "            # Perform a single optimization step (parameter update).\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Aggregate the loss for the epoch\n",
    "            loss_epoch += float(loss.item())\n",
    "            num_batches += 1\n",
    "            \n",
    "            predictions.extend(pred_mass.detach().cpu().numpy().flatten())\n",
    "            ground_truths.extend(gt_mass.detach().cpu().numpy().flatten())\n",
    "            pbar.set_description(\"Loss: {:.4f}\".format(loss.item()))\n",
    "            \n",
    "        pbar.close()\n",
    "        avg_loss = loss_epoch / num_batches\n",
    "        self.train_loss.append(avg_loss)\n",
    "        \n",
    "        r2 = r2_score(ground_truths, predictions)\n",
    "        r = r_score(ground_truths, predictions)\n",
    "        rmse = sqrt(mean_squared_error(ground_truths, predictions))\n",
    "        \n",
    "        writer.add_scalar(\"Training/Overall_Loss\", avg_loss, self.cur_epoch)\n",
    "        writer.add_scalar(\"Training/Overall_R\", r, self.cur_epoch)\n",
    "        writer.add_scalar(\"Training/Overall_R2\", r2, self.cur_epoch)\n",
    "        writer.add_scalar(\"Training/Overall_RMSE\", rmse, self.cur_epoch)\n",
    "        \n",
    "        np_predictions = np.array(predictions)\n",
    "        np_ground_truths = np.array(ground_truths)\n",
    "        overall_rrmse = relative_root_mean_squared_error(np_predictions, np_ground_truths)\n",
    "        writer.add_scalar(\"Training/Overall_Relative_RMSE\", overall_rrmse, self.cur_epoch)\n",
    "\n",
    "        self.cur_epoch += 1\n",
    "        pbar.set_description(\"Epoch: {}, Average Loss: {:.4f}\".format(self.cur_epoch, avg_loss))\n",
    "\n",
    "    def test(self, dataloader_test):\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        loss_epoch = 0.\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Prepare to collect predictions and ground truth\n",
    "        predictions = []\n",
    "        ground_truths = []\n",
    "        \n",
    "        with torch.no_grad():  # No need to calculate gradients\n",
    "            pbar = tqdm(dataloader_test, desc='Test Epoch {}'.format(self.cur_epoch))\n",
    "            for data in pbar:\n",
    "                images = data[0].to(self.device, dtype=torch.float32)\n",
    "                gt_mass = data[1].to(self.device, dtype=torch.float32)\n",
    "\n",
    "                pred_mass = self.model(images)\n",
    "\n",
    "                # The output should have the same dimensions as the target\n",
    "                if pred_mass.ndim > gt_mass.ndim:\n",
    "                    pred_mass = pred_mass.squeeze()\n",
    "\n",
    "                loss = self.criterion(pred_mass, gt_mass)\n",
    "                loss_epoch += float(loss.item())\n",
    "                num_batches += 1\n",
    "                \n",
    "                # We want to put this back on the CPU to calculate the metrics\n",
    "                predictions.extend(pred_mass.cpu().numpy().flatten())\n",
    "                ground_truths.extend(gt_mass.cpu().numpy().flatten())\n",
    "                pbar.set_description(\"Test Loss: {:.4f}\".format(loss.item()))\n",
    "\n",
    "        avg_loss = loss_epoch / num_batches\n",
    "        self.val_loss.append(avg_loss)\n",
    "        \n",
    "        # Calculate the R2 score, R score, and RMSE for overall test set\n",
    "        r2 = r2_score(ground_truths, predictions)\n",
    "        r = r_score(ground_truths, predictions)\n",
    "        rmse = sqrt(mean_squared_error(ground_truths, predictions))\n",
    "        \n",
    "        print(f\"Test Epoch: {self.cur_epoch}, Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"R2 Score: {r2:.4f}, R Score: {r:.4f}, RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        writer.add_scalar(\"Testing/Overall_Loss\", avg_loss, self.cur_epoch)\n",
    "        writer.add_scalar(\"Testing/Overall_R2_Score\", r2, self.cur_epoch)\n",
    "        writer.add_scalar(\"Testing/Overall_R_Score\", r, self.cur_epoch)\n",
    "        writer.add_scalar(\"Testing/Overall_RMSE\", rmse, self.cur_epoch)\n",
    "        \n",
    "        np_predictions = np.array(predictions)\n",
    "        np_ground_truths = np.array(ground_truths)\n",
    "        overall_rrmse = relative_root_mean_squared_error(np_predictions, np_ground_truths)\n",
    "        writer.add_scalar(\"Testing/Overall_Relative_RMSE\", overall_rrmse, self.cur_epoch)\n",
    "        \n",
    "        print(\"{:<10} {:<15} {:<15} {:<15}\".format('Sample', 'Predicted', 'GT', 'Relative RMSE'))\n",
    "        for sample_num, (pred, gt) in enumerate(zip(predictions, ground_truths), 1):\n",
    "            # Calculate the relative RMSE for each instances\n",
    "            relative_rmse = relative_root_mean_squared_error(pred, gt)\n",
    "            writer.add_scalar(f\"Testing/Relative_RMSE/Sample_{sample_num}\", relative_rmse, self.cur_epoch)\n",
    "            writer.add_scalars(f'TestingPredictionvsGT/Sample_{sample_num}', {\n",
    "                'Prediction': pred,\n",
    "                'Ground Truth': gt,\n",
    "            }, self.cur_epoch)\n",
    "            print(\"{:<10} {:<15.5f} {:<15.5f} {:<15.5f}\".format(sample_num, pred, gt, relative_rmse))\n",
    "\n",
    "        return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cqilab/anaconda3/envs/sam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Total trainable parameters:  149084442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 0:   0%|          | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 382.6480: 100%|██████████| 98/98 [00:13<00:00,  7.26it/s] \n",
      "Test Loss: 3926.4768: 100%|██████████| 2/2 [00:00<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 1, Average Loss: 2222.4234\n",
      "R2 Score: -0.6077, R Score: 0.5364, RMSE: 45.2343\n",
      "Sample     Predicted       GT              Relative RMSE  \n",
      "1          4.87615         5.04922         0.03428        \n",
      "2          5.14825         14.22000        0.63796        \n",
      "3          4.48482         0.07432         59.34201       \n",
      "4          4.39954         0.14031         30.35621       \n",
      "5          4.77615         2.56000         0.86568        \n",
      "6          5.40714         75.56802        0.92845        \n",
      "7          5.40222         32.32000        0.83285        \n",
      "8          3.93913         0.98394         3.00343        \n",
      "9          4.95987         7.36000         0.32610        \n",
      "10         5.86028         53.32000        0.89009        \n",
      "11         4.74894         3.57000         0.33023        \n",
      "12         4.86315         3.85000         0.26316        \n",
      "13         5.42863         13.57483        0.60010        \n",
      "14         4.21102         0.22750         17.50974       \n",
      "15         4.07787         16.74877        0.75653        \n",
      "16         5.36056         8.50000         0.36935        \n",
      "17         5.63988         87.87778        0.93582        \n",
      "18         5.43966         128.67818       0.95773        \n",
      "19         5.92071         100.05196       0.94082        \n",
      "20         5.74540         41.73172        0.86233        \n",
      "21         4.50695         66.37000        0.93209        \n",
      "22         4.78158         11.76000        0.59340        \n",
      "23         5.90381         9.64813         0.38809        \n",
      "24         6.09959         22.78601        0.73231        \n",
      "25         5.88924         84.90561        0.93064        \n",
      "26         4.23713         49.50000        0.91440        \n",
      "27         5.90404         64.84397        0.90895        \n",
      "28         4.09326         0.67137         5.09692        \n",
      "29         6.07407         60.76912        0.90005        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 362.8625: 100%|██████████| 98/98 [00:13<00:00,  7.32it/s] \n",
      "Test Loss: 269.8410: 100%|██████████| 2/2 [00:00<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 2, Average Loss: 355.5290\n",
      "R2 Score: 0.7137, R Score: 0.8944, RMSE: 19.0891\n",
      "Sample     Predicted       GT              Relative RMSE  \n",
      "1          7.75148         5.04922         0.53518        \n",
      "2          56.81718        128.67818       0.55846        \n",
      "3          14.03964        14.22000        0.01268        \n",
      "4          54.28704        64.84397        0.16281        \n",
      "5          47.68707        32.32000        0.47547        \n",
      "6          8.12434         8.50000         0.04420        \n",
      "7          0.35654         3.57000         0.90013        \n",
      "8          14.26142        13.57483        0.05058        \n",
      "9          47.21222        49.50000        0.04622        \n",
      "10         54.15566        84.90561        0.36217        \n",
      "11         0.76786         0.98394         0.21960        \n",
      "12         0.25827         0.22750         0.13524        \n",
      "13         1.41427         3.85000         0.63266        \n",
      "14         31.39013        16.74877        0.87418        \n",
      "15         56.64641        75.56802        0.25039        \n",
      "16         1.90352         2.56000         0.25644        \n",
      "17         49.85541        66.37000        0.24883        \n",
      "18         57.45792        100.05196       0.42572        \n",
      "19         5.23414         0.07432         69.42392       \n",
      "20         8.15088         7.36000         0.10746        \n",
      "21         56.94328        60.76912        0.06296        \n",
      "22         56.12235        87.87778        0.36136        \n",
      "23         33.32095        22.78601        0.46234        \n",
      "24         54.30463        41.73172        0.30128        \n",
      "25         9.50954         9.64813         0.01436        \n",
      "26         4.39941         0.14031         30.35530       \n",
      "27         20.42707        11.76000        0.73700        \n",
      "28         50.45834        53.32000        0.05367        \n",
      "29         1.02602         0.67137         0.52825        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 13.4573: 100%|██████████| 98/98 [00:13<00:00,  7.32it/s] \n",
      "Test Loss: 114.1587: 100%|██████████| 2/2 [00:00<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 3, Average Loss: 155.4170\n",
      "R2 Score: 0.8745, R Score: 0.9480, RMSE: 12.6367\n",
      "Sample     Predicted       GT              Relative RMSE  \n",
      "1          53.64182        53.32000        0.00604        \n",
      "2          10.56074        7.36000         0.43488        \n",
      "3          4.65786         13.57483        0.65687        \n",
      "4          72.15982        60.76912        0.18744        \n",
      "5          0.29307         0.67137         0.56347        \n",
      "6          18.24424        22.78601        0.19932        \n",
      "7          6.06185         8.50000         0.28684        \n",
      "8          75.79320        128.67818       0.41099        \n",
      "9          0.29290         0.98394         0.70232        \n",
      "10         11.65119        16.74877        0.30436        \n",
      "11         0.31528         0.14031         1.24707        \n",
      "12         0.32191         2.56000         0.87425        \n",
      "13         0.41599         3.57000         0.88348        \n",
      "14         0.31136         0.22750         0.36862        \n",
      "15         0.43247         3.85000         0.88767        \n",
      "16         39.42803        32.32000        0.21993        \n",
      "17         68.80046        64.84397        0.06102        \n",
      "18         8.30047         11.76000        0.29418        \n",
      "19         73.41611        75.56802        0.02848        \n",
      "20         8.95162         5.04922         0.77287        \n",
      "21         73.78741        87.87778        0.16034        \n",
      "22         8.24625         14.22000        0.42010        \n",
      "23         56.84382        41.73172        0.36213        \n",
      "24         68.60369        84.90561        0.19200        \n",
      "25         61.19096        66.37000        0.07803        \n",
      "26         43.82633        49.50000        0.11462        \n",
      "27         75.57413        100.05196       0.24465        \n",
      "28         0.31368         0.07432         3.22050        \n",
      "29         2.57747         9.64813         0.73285        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 301.2220: 100%|██████████| 98/98 [00:13<00:00,  7.31it/s]\n",
      "Test Loss: 671.3058: 100%|██████████| 2/2 [00:00<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 4, Average Loss: 395.5724\n",
      "R2 Score: 0.7116, R Score: 0.9092, RMSE: 19.1585\n",
      "Sample     Predicted       GT              Relative RMSE  \n",
      "1          54.08938        84.90561        0.36295        \n",
      "2          1.11138         0.14031         6.92099        \n",
      "3          43.14389        32.32000        0.33490        \n",
      "4          55.11437        60.76912        0.09305        \n",
      "5          3.42165         3.57000         0.04155        \n",
      "6          46.46347        53.32000        0.12859        \n",
      "7          9.47542         8.50000         0.11476        \n",
      "8          1.12111         0.22750         3.92791        \n",
      "9          1.28512         2.56000         0.49800        \n",
      "10         7.64084         16.74877        0.54380        \n",
      "11         1.14046         0.07432         14.34463       \n",
      "12         33.46968        22.78601        0.46887        \n",
      "13         54.09190        64.84397        0.16581        \n",
      "14         2.07529         5.04922         0.58899        \n",
      "15         12.44258        13.57483        0.08341        \n",
      "16         54.51255        75.56802        0.27863        \n",
      "17         51.34261        41.73172        0.23030        \n",
      "18         47.16494        49.50000        0.04717        \n",
      "19         55.98322        100.05196       0.44046        \n",
      "20         5.05666         3.85000         0.31342        \n",
      "21         54.56618        87.87778        0.37907        \n",
      "22         1.24464         0.67137         0.85389        \n",
      "23         55.63735        128.67818       0.56762        \n",
      "24         52.59052        66.37000        0.20762        \n",
      "25         7.91801         11.76000        0.32670        \n",
      "26         12.40160        9.64813         0.28539        \n",
      "27         10.58014        7.36000         0.43752        \n",
      "28         1.32090         0.98394         0.34247        \n",
      "29         9.97863         14.22000        0.29827        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 104.1218: 100%|██████████| 98/98 [00:13<00:00,  7.32it/s]\n",
      "Test Loss: 186.6201: 100%|██████████| 2/2 [00:00<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 5, Average Loss: 434.5212\n",
      "R2 Score: 0.6384, R Score: 0.9201, RMSE: 21.4515\n",
      "Sample     Predicted       GT              Relative RMSE  \n",
      "1          41.85197        53.32000        0.21508        \n",
      "2          14.52910        11.76000        0.23547        \n",
      "3          59.50224        128.67818       0.53759        \n",
      "4          0.58860         0.98394         0.40179        \n",
      "5          4.87476         5.04922         0.03455        \n",
      "6          14.05226        8.50000         0.65321        \n",
      "7          33.95779        84.90561        0.60005        \n",
      "8          0.29855         0.07432         3.01688        \n",
      "9          51.72692        100.05196       0.48300        \n",
      "10         11.88532        7.36000         0.61485        \n",
      "11         14.68917        13.57483        0.08209        \n",
      "12         29.90397        32.32000        0.07475        \n",
      "13         42.87827        66.37000        0.35395        \n",
      "14         46.72126        60.76912        0.23117        \n",
      "15         0.44803         0.67137         0.33266        \n",
      "16         59.58683        75.56802        0.21148        \n",
      "17         36.46780        49.50000        0.26328        \n",
      "18         0.29501         0.14031         1.10259        \n",
      "19         15.47609        9.64813         0.60405        \n",
      "20         49.70216        41.73172        0.19099        \n",
      "21         0.86983         2.56000         0.66022        \n",
      "22         0.30079         0.22750         0.32215        \n",
      "23         53.69748        87.87778        0.38895        \n",
      "24         5.94345         3.57000         0.66483        \n",
      "25         12.26984        16.74877        0.26742        \n",
      "26         34.50962        64.84397        0.46781        \n",
      "27         13.84846        14.22000        0.02613        \n",
      "28         23.68020        22.78601        0.03924        \n",
      "29         10.23324        3.85000         1.65799        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 336.3978:  35%|███▍      | 34/98 [00:05<00:09,  6.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m======Total trainable parameters: \u001b[39m\u001b[38;5;124m'\u001b[39m, params)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mcur_epoch, training_epoch):\n\u001b[0;32m---> 26\u001b[0m \t\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \t\u001b[38;5;66;03m# Test the model every 20 epochs and save it to logs folder\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[9], line 54\u001b[0m, in \u001b[0;36mEngine.train\u001b[0;34m(self, dataloader_train)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Aggregate the loss for the epoch\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     55\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     57\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend(pred_mass\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from codes.scheduler import CyclicCosineDecayLR\n",
    "\n",
    "# The scheduler is used to reduce the learning rate at specific epochs\n",
    "if scheduler:\n",
    "\tscheduler = CyclicCosineDecayLR(optimizer,\n",
    "\t                                init_decay_epochs=150,\n",
    "\t                                min_decay_lr=2.5e-6,\n",
    "\t                                restart_interval = 10,\n",
    "\t                                restart_lr=12.5e-5,\n",
    "\t                                warmup_epochs=20,\n",
    "\t                                warmup_start_lr=2.5e-6)\n",
    "\n",
    "\n",
    "trainer = Engine(model, optimizer, device, ema=None)\n",
    "\n",
    "# Load the saved model if load_saved_model is set to True\n",
    "if load_saved_model:\n",
    "\tmodel.load_state_dict(torch.load('logs/final_model.pth'))\n",
    " \n",
    "# Count the total number of trainable parameters\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print ('======Total trainable parameters: ', params)\n",
    "\n",
    "for epoch in range(trainer.cur_epoch, training_epoch):\n",
    "\ttrainer.train(dataloader_train)\n",
    "\n",
    "\t# Test the model every 20 epochs and save it to logs folder\n",
    "\tif (epoch + 1) % 20 == 0:\n",
    "\t\ttrainer.test(dataloader_test)\n",
    "\t\ttorch.save(model.state_dict(), os.path.join('logs', 'final_model.pth'))\n",
    "\tif scheduler:\n",
    "\t\tscheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join('logs', 'final_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'writer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m      2\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'writer' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
